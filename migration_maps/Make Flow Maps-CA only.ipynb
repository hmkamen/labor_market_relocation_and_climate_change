{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f40966",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d679d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp0_gr=tmp0.groupby(['msa','statefip'],as_index=False).agg({'id':sum})\n",
    "# bmk0_gr=bmk0.groupby(['msa'],as_index=False).agg({'id':sum})\n",
    "# msaid_state_lookup=tmp0_gr[['msa','statefip']].merge(bmk0_gr[['msa']],on='msa').groupby(['msa','statefip'],as_index=False).sum()\n",
    "# msaid_state_lookup.to_excel('/Users/hannahkamen/Downloads/msaid_state_lookup.xlsx')\n",
    "\n",
    "#####get MSA Identifier information\n",
    "# msa_id=pd.read_stata('/Users/hannahkamen/Downloads/population-migration-master/estimation/1_main_specification/acs5yr0610/dta/msa_identifier.dta')\n",
    "# msa_vars=pd.read_stata('/Users/hannahkamen/Downloads/population-migration-master/estimation/1_main_specification/acs5yr0610/dta/second_stage_dataset_cl.dta')\n",
    "state_split=pd.read_excel('/Users/hannahkamen/Downloads/state_pop_educ_shares.xlsx')\n",
    "sl=pd.read_excel('/Users/hannahkamen/Downloads/statelookup2.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e469be9",
   "metadata": {},
   "source": [
    "##### (2) get coordinate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7e17ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "###prepare dataset with centroid of every state\n",
    "\n",
    "map_dta1=pd.read_csv('/Users/hannahkamen/Downloads/census_texas.csv')\n",
    "map_dta2=pd.read_csv('/Users/hannahkamen/Downloads/census_ny.csv')\n",
    "map_dta=map_dta1.append(map_dta2).reset_index()\n",
    "\n",
    "###rename data\n",
    "map_dta['centroid2']=map_dta['centroid1']+\",\"+map_dta['centroid2']\n",
    "map_dta['centroid1']=map_dta['FULL1_NAME']+\",\"+map_dta['FULL2_NAME']\n",
    "map_dta['FULL1_NAME']=map_dta['GEOID1']\n",
    "map_dta['FULL2_NAME']=map_dta['GEOID2']\n",
    "\n",
    "del map_dta['GEOID1']\n",
    "del map_dta['GEOID2']\n",
    "\n",
    "\n",
    "map_dta['GEOID1']=map_dta['level_1']\n",
    "map_dta['GEOID2']=map_dta['Unnamed: 0']\n",
    "map_dta=map_dta[['GEOID1','GEOID2','FULL1_NAME','FULL2_NAME','centroid1','centroid2']]\n",
    "####get abbreviations\n",
    "\n",
    "states=list(sl['abbrev'].unique())\n",
    "\n",
    "state_coords=pd.DataFrame()\n",
    "for s in states:\n",
    "    tmp=map_dta[map_dta['FULL2_NAME'].str.contains(s)]\n",
    "    tmp['state']=s\n",
    "    state_coords=state_coords.append(tmp)\n",
    "state_coords=state_coords.drop_duplicates(subset='state')\n",
    "state_coords=state_coords[['state','FULL2_NAME','centroid2']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877bf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_coords['id']= np.arange(len(state_coords))\n",
    "state_coords['name']=state_coords['state']\n",
    "state_coords['lat']=state_coords['centroid2'].apply(lambda x: x.split(',')[1].replace(\")\",'').strip())\n",
    "state_coords['lon']=state_coords['centroid2'].apply(lambda x: x.split(',')[0].replace(\"(\",'').replace(\"c\",'').strip())\n",
    "state_coords_lm=state_coords[['id','name','lat','lon']]\n",
    "state_coords_lm.to_csv('/Users/hannahkamen/Downloads/flowmap_location_lookup.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7372b6c7",
   "metadata": {},
   "source": [
    "##### (2) map out migration flows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78122c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####import state pop shares by skilled and unskilled\n",
    "state_educ=pd.read_excel('/Users/hannahkamen/Downloads/state_educ_shares.xlsx')\n",
    "del state_educ['statefip']\n",
    "state_educ=state_educ.merge(sl,on='state',how='inner')\n",
    "state_educ=state_educ[['statefip','skl','unskl','state','abbrev','state_pop']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02376ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "###these are the population shares changes from the first cycle of GAMS data inputs (after initial logit shock with temp)\n",
    "###the shares caluclated here represent the population change from the previous iteration \n",
    "state_split=pd.read_excel('/Users/hannahkamen/Downloads/state_pop_educ_shares.xlsx')\n",
    "\n",
    "msaid_state_lookup=pd.read_excel('/Users/hannahkamen/Downloads/msaid_state_lookup.xlsx')\n",
    "sl=pd.read_excel('/Users/hannahkamen/Downloads/statelookup2.xlsx')\n",
    "tmp0=pd.read_stata('/Users/hannahkamen/Downloads/population-migration-master/estimation/1_main_specification/acs5yr0610/dta/projection_data_age2_01_wbmk_v2_0_ca_only.dta')\n",
    "tmp0=tmp0.merge(sl,on='statefip')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e4248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ####store in csv\n",
    "\n",
    "master_map_unskl2=tmp0[tmp0['coll']==0]\n",
    "master_map_skl2=tmp0[tmp0['coll'] ==1]\n",
    "\n",
    "del tmp0\n",
    "\n",
    "master_map_unskl2.to_csv('/Users/hannahkamen/Downloads/map_data_unskl2_ca_only.csv')\n",
    "del master_map_unskl2\n",
    "master_map_skl2.to_csv('/Users/hannahkamen/Downloads/map_data_skl2_ca_only.csv')\n",
    "del master_map_skl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e3f2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_map_unskl2=pd.read_csv('/Users/hannahkamen/Downloads/map_data_unskl2_ca_only.csv')\n",
    "master_map_skl2=pd.read_csv('/Users/hannahkamen/Downloads/map_data_skl2_ca_only.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2912f4",
   "metadata": {},
   "source": [
    "#### Unskilled Map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74bf809",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### prep data for map\n",
    "\n",
    "#get sum of shares in all other states for people who chose states 1-44\n",
    "#get ids that live in state s currentlly\n",
    "unskl=pd.DataFrame()\n",
    "for s in master_map_unskl2['state'].unique():\n",
    "    living_now=[]\n",
    "    tmp1=master_map_unskl2[(master_map_unskl2['state']==s) & (master_map_unskl2['chosen']==1)]\n",
    "    ####list of ids currently living in state s\n",
    "    living_now.append(list(tmp1['id'].unique())[0])\n",
    "    ###limit dataframe to the ids of people living in state s\n",
    "    tmp2=master_map_unskl2[master_map_unskl2['id'].isin(living_now)]\n",
    "    ####now groupby shares across all states\n",
    "    tmp3=tmp2.groupby(['state'],as_index=False).agg({'share0':sum,'shareb':sum})\n",
    "    ###tag origin state\n",
    "    tmp3['living_flag']=s\n",
    "    unskl=unskl.append(tmp3)\n",
    "    \n",
    "######merge with state populaiton\n",
    "unskl=unskl.merge(state_split,left_on='living_flag',right_on='state',how='inner')\n",
    "unskl=unskl.rename(columns={'state_x':'moving_to'})\n",
    "\n",
    "######get share difference between current  iteration and benchmark\n",
    "unskl['share_diff']=unskl['share0']-unskl['shareb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c7f89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(master_map_unskl2[master_map_unskl2['chosen']==1]['state'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c16fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### prep data for map\n",
    "\n",
    "#get sum of shares in all other states for people who chose states 1-44\n",
    "#get ids that live in state s currentlly\n",
    "skl=pd.DataFrame()\n",
    "for s in master_map_skl2['state'].unique():\n",
    "    living_now=[]\n",
    "    tmp1=master_map_skl2[(master_map_skl2['state']==s) & (master_map_skl2['chosen']==1)]\n",
    "    ####list of ids currently living in state s\n",
    "    living_now.append(list(tmp1['id'].unique())[0])\n",
    "    ###limit dataframe to the ids of people living in state s\n",
    "    tmp2=master_map_skl2[master_map_skl2['id'].isin(living_now)]\n",
    "    ####now groupby shares across all states\n",
    "    tmp3=tmp2.groupby(['state'],as_index=False).agg({'share0':sum,'shareb':sum})\n",
    "    ###tag origin state\n",
    "    tmp3['living_flag']=s\n",
    "    skl=skl.append(tmp3)\n",
    "    \n",
    "######merge with state populaiton\n",
    "skl=skl.merge(state_split,left_on='living_flag',right_on='state',how='inner')\n",
    "skl=skl.rename(columns={'state_x':'moving_to'})\n",
    "\n",
    "######get share difference between current  iteration and benchmark\n",
    "skl['share_diff']=skl['share0']-skl['shareb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb669b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "####read in pct changes by state and education\n",
    "le0=pd.read_csv('/Users/hannahkamen/Downloads/le0_shock0_v2_test2_ncollege_only.csv')\n",
    "\n",
    "le0=le0[['r','sk','skill_shr']]\n",
    "le0=le0.drop_duplicates(subset=['r','sk'])\n",
    "le0=le0.rename(columns={'sk':'educ_id','skill_shr':'final_change','r':'abbrev'})\n",
    "le0=le0.merge(sl,on='abbrev')\n",
    "le0_skl=le0[le0['educ_id']=='skl']\n",
    "le0_unskl=le0[le0['educ_id']=='unskl']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c59f0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare pop map data for skilled entrance\n",
    "all_skl_entrance=pd.DataFrame()\n",
    "\n",
    "for s in le0_skl[le0_skl['final_change']>0]['state'].unique():\n",
    "    ###for each entrance state, calculate shares by origin state that make up total change in the positive share diff\n",
    "    master_map_skl_lm=skl[((skl['moving_to']==s) & (skl['share_diff']>0))]\n",
    "    #####get totals \n",
    "    master_map_skl_lm_totalinflows=master_map_skl_lm.groupby('moving_to').agg({'share_diff':sum})\n",
    "    ###rename and merge\n",
    "    master_map_skl_lm_totalinflows=master_map_skl_lm_totalinflows.rename(columns={'share_diff':'share_diff_tot'})\n",
    "    master_map_skl_lm=master_map_skl_lm.merge(master_map_skl_lm_totalinflows,on='moving_to')\n",
    "    \n",
    "    master_map_skl_lm['inflow_share']= master_map_skl_lm['share_diff']/master_map_skl_lm['share_diff_tot']\n",
    "    \n",
    "    master_map_skl_lm=master_map_skl_lm[['moving_to','living_flag','inflow_share','share_diff_tot']]\n",
    "    all_skl_entrance=all_skl_entrance.append(master_map_skl_lm)\n",
    "    \n",
    "all_skl_entrance=all_skl_entrance.merge(state_educ,left_on='moving_to',right_on='state').merge(le0_skl,on='state')\n",
    "all_skl_entrance['raw_pop_change_moving_to']=all_skl_entrance['final_change']*all_skl_entrance['state_pop']*all_skl_entrance['skl']*all_skl_entrance['inflow_share']\n",
    "\n",
    "##merge in pop info\n",
    "\n",
    "all_skl_entrance=all_skl_entrance.rename(columns={'abbrev_x':'dest'})\n",
    "all_skl_entrance=all_skl_entrance.merge(sl[['state','abbrev']],left_on='living_flag',right_on='state')\n",
    "all_skl_entrance=all_skl_entrance.rename(columns={'abbrev':'origin'})\n",
    "\n",
    "all_skl_entrance=all_skl_entrance.merge(state_coords_lm,left_on='dest',right_on='name')\n",
    "del all_skl_entrance['dest']\n",
    "all_skl_entrance['dest']=all_skl_entrance['id']\n",
    "del all_skl_entrance['id']\n",
    "all_skl_entrance=all_skl_entrance.merge(state_coords_lm,left_on='origin',right_on='name')\n",
    "del all_skl_entrance['origin']\n",
    "\n",
    "\n",
    "# all_skl_entrance=all_skl_entrance.merge(state_educ,left_on='moving_to',right_on='state').merge(total_skilled_pop_changes,left_on='moving_to',right_on='statefip')\n",
    "all_skl_entrance['origin']=all_skl_entrance['id']\n",
    "all_skl_entrance['count']=all_skl_entrance['raw_pop_change_moving_to']\n",
    "\n",
    "all_skl_entrance_lm=all_skl_entrance[['origin','dest','count']]\n",
    "all_skl_entrance_lm['count']=all_skl_entrance_lm['count']/14412444.2\n",
    "all_skl_entrance_lm.to_csv('/Users/hannahkamen/Downloads/skilled_entrance_ca_only.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379241f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####prepare pop map data for skilled entrance\n",
    "all_unskl_entrance=pd.DataFrame()\n",
    "\n",
    "for s in le0_unskl[le0_unskl['final_change']>0]['state'].unique():\n",
    "    ###for each entrance state, calculate shares by origin state that make up total change in the positive share diff\n",
    "    master_map_unskl_lm=unskl[((unskl['moving_to']==s) & (unskl['share_diff']>0))]\n",
    "    #####get totals \n",
    "    master_map_unskl_lm_totalinflows=master_map_unskl_lm.groupby('moving_to').agg({'share_diff':sum})\n",
    "    ###rename and merge\n",
    "    master_map_unskl_lm_totalinflows=master_map_unskl_lm_totalinflows.rename(columns={'share_diff':'share_diff_tot'})\n",
    "    master_map_unskl_lm=master_map_unskl_lm.merge(master_map_unskl_lm_totalinflows,on='moving_to')\n",
    "    \n",
    "    master_map_unskl_lm['inflow_share']= master_map_unskl_lm['share_diff']/master_map_unskl_lm['share_diff_tot']\n",
    "    \n",
    "    master_map_unskl_lm=master_map_unskl_lm[['moving_to','living_flag','inflow_share','share_diff_tot']]\n",
    "    all_unskl_entrance=all_unskl_entrance.append(master_map_unskl_lm)\n",
    "    \n",
    "all_unskl_entrance=all_unskl_entrance.merge(state_educ,left_on='moving_to',right_on='state').merge(le0_unskl,on='state')\n",
    "all_unskl_entrance['raw_pop_change_moving_to']=all_unskl_entrance['final_change']*all_unskl_entrance['state_pop']*all_unskl_entrance['unskl']*all_unskl_entrance['inflow_share']\n",
    "\n",
    "##merge in pop info\n",
    "\n",
    "all_unskl_entrance=all_unskl_entrance.rename(columns={'abbrev_x':'dest'})\n",
    "all_unskl_entrance=all_unskl_entrance.merge(sl[['state','abbrev']],left_on='living_flag',right_on='state')\n",
    "all_unskl_entrance=all_unskl_entrance.rename(columns={'abbrev':'origin'})\n",
    "\n",
    "all_unskl_entrance=all_unskl_entrance.merge(state_coords_lm,left_on='dest',right_on='name')\n",
    "del all_unskl_entrance['dest']\n",
    "all_unskl_entrance['dest']=all_unskl_entrance['id']\n",
    "del all_unskl_entrance['id']\n",
    "all_unskl_entrance=all_unskl_entrance.merge(state_coords_lm,left_on='origin',right_on='name')\n",
    "del all_unskl_entrance['origin']\n",
    "\n",
    "\n",
    "# all_unskl_entrance=all_unskl_entrance.merge(state_educ,left_on='moving_to',right_on='state').merge(total_skilled_pop_changes,left_on='moving_to',right_on='statefip')\n",
    "all_unskl_entrance['origin']=all_unskl_entrance['id']\n",
    "all_unskl_entrance['count']=all_unskl_entrance['raw_pop_change_moving_to']\n",
    "\n",
    "all_unskl_entrance_lm=all_unskl_entrance[['origin','dest','count']]\n",
    "\n",
    "all_unskl_entrance_lm['count']=all_unskl_entrance_lm['count']/14412444.2\n",
    "\n",
    "all_unskl_entrance_lm.to_csv('/Users/hannahkamen/Downloads/unskilled_entrance_ca_only.csv',index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac292be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
